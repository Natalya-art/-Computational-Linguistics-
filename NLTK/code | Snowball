import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import SnowballStemmer

text = '''
С НОВЫМ ГОДОМ, ЛЮБИМЫЕ!
Спасибо за этот пусть и тяжёлый 2020-ый. Вы лучшая аудитория!
Пусть ваши мечты сбываются, йоу)
Вот вам подарочек в виде двух песенок)
'''

tokens = word_tokenize(text)
print('Результат токенизации на слова:')
print(tokens)
print('...............................................................................................')
stems = []
stemmer = SnowballStemmer("russian")     #Стеммер Snowball
for token in tokens:
    token = stemmer.stem(token)
    if token != "":
        stems.append(token)
print("stems:", stems)



RESULT:
Результат токенизации на слова:
['С', 'НОВЫМ', 'ГОДОМ', ',', 'ЛЮБИМЫЕ', '!', 'Спасибо', 'за', 'этот', 'пусть', 'и', 'тяжёлый', '2020-ый', '.', 'Вы',
'лучшая', 'аудитория', '!', 'Пусть', 'ваши', 'мечты', 'сбываются', ',', 'йоу', ')', 'Вот', 'вам', 'подарочек', 'в',
'виде', 'двух', 'песенок', ')']
...............................................................................................
stems: ['с', 'нов', 'год', ',', 'любим', '!', 'спасиб', 'за', 'этот', 'пуст', 'и', 'тяжел', '2020-ы', '.', 'вы',
'лучш', 'аудитор', '!', 'пуст', 'ваш', 'мечт', 'сбыва', ',', 'йо', ')', 'вот', 'вам', 'подарочек', 'в', 'вид',
'двух', 'песенок', ')']
